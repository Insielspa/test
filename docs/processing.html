<!DOCTYPE html SYSTEM "about:legacy-compat">
<html lang="en-US" data-preset="contrast" data-primary-color="#307FFF"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="robots" content="noindex"><meta name="built-on" content="2024-11-14T16:15:54.639494877"><title>Processing | FVG Vision AI</title><script type="application/json" id="virtual-toc-data">[{"id":"description","level":0,"title":"Description","anchor":"#description"},{"id":"functionality","level":0,"title":"Functionality","anchor":"#functionality"},{"id":"example-usage","level":0,"title":"Example Usage","anchor":"#example-usage"},{"id":"notes","level":0,"title":"Notes","anchor":"#notes"}]</script><script type="application/json" id="topic-shortcuts"></script><link href="https://resources.jetbrains.com/writerside/apidoc/6.10.0-b518/app.css" rel="stylesheet"><meta name="msapplication-TileColor" content="#000000"><link rel="apple-touch-icon" sizes="180x180" href="https://jetbrains.com/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="https://jetbrains.com/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="https://jetbrains.com/favicon-16x16.png"><meta name="msapplication-TileImage" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-144x144.png"><meta name="msapplication-square70x70logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-70x70.png"><meta name="msapplication-square150x150logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-150x150.png"><meta name="msapplication-wide310x150logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-310x150.png"><meta name="msapplication-square310x310logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-310x310.png"><meta name="image" content=""><!-- Open Graph --><meta property="og:title" content="Processing | FVG Vision AI"><meta property="og:description" content=""><meta property="og:image" content=""><meta property="og:site_name" content="FVG Vision AI Help"><meta property="og:type" content="website"><meta property="og:locale" content="en_US"><meta property="og:url" content="writerside-documentation/processing.html"><!-- End Open Graph --><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content=""><meta name="twitter:title" content="Processing | FVG Vision AI"><meta name="twitter:description" content=""><meta name="twitter:creator" content=""><meta name="twitter:image:src" content=""><!-- End Twitter Card --><!-- Schema.org WebPage --><script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": "WebPage",
    "@id": "writerside-documentation/processing.html#webpage",
    "url": "writerside-documentation/processing.html",
    "name": "Processing | FVG Vision AI",
    "description": "",
    "image": "",
    "inLanguage":"en-US"
}</script><!-- End Schema.org --><!-- Schema.org WebSite --><script type="application/ld+json">{
    "@type": "WebSite",
    "@id": "writerside-documentation/#website",
    "url": "writerside-documentation/",
    "name": "FVG Vision AI Help"
}</script><!-- End Schema.org --></head><body data-id="Processing" data-main-title="Processing" data-article-props="{&quot;seeAlsoStyle&quot;:&quot;links&quot;}" data-template="article" data-breadcrumbs="Main-modules.md|Application Architecture"><div class="wrapper"><main class="panel _main"><header class="panel__header"><div class="container"><h3>FVG Vision AI  Help</h3><div class="panel-trigger"></div></div></header><section class="panel__content"><div class="container"><article class="article" data-shortcut-switcher="inactive"><h1 data-toc="Processing" id="Processing.md">Processing</h1><section class="chapter"><h2 id="description" data-toc="description">Description</h2><p id="z6sdncg_7">The <code class="code" id="z6sdncg_8">UltralyticsProcessor</code> class is designed to process video streams using the Ultralytics YOLO model. It handles both network streams and local video files, providing a unified interface for object detection and analysis.</p></section><section class="chapter"><h2 id="functionality" data-toc="functionality">Functionality</h2><section class="chapter"><h3 id="initialization" data-toc="initialization">Initialization</h3><p id="z6sdncg_14">The class initializes by loading the YOLO model based on the provided settings. It configures various parameters such as model dimensions, tracking options, and scenario-specific settings (e.g., pose detection, zone monitoring, door monitoring, and parking monitoring).</p></section><section class="chapter"><h3 id="video-source-setup" data-toc="video-source-setup">Video Source Setup</h3><p id="z6sdncg_15">When a video source is provided, the class sets up the video parameters, including the source dimensions and frame rate. It adjusts the model's input size to be compatible with the source video and initializes various decorators for drawing information on the frames.</p></section><section class="chapter"><h3 id="frame-analysis" data-toc="frame-analysis">Frame Analysis</h3><p id="z6sdncg_16">The core functionality of the class revolves around analyzing each frame of the video stream. For each frame, the following steps are performed:</p><ol class="list _decimal" id="z6sdncg_17" type="1"><li class="list__item" id="z6sdncg_18"><p><span class="control" id="z6sdncg_24">Frame Resizing</span>: The source frame is resized to match the model's input dimensions.</p></li><li class="list__item" id="z6sdncg_19"><p><span class="control" id="z6sdncg_25">Object Detection</span>: The YOLO model processes the resized frame to detect objects. If tracking is enabled, it also tracks the detected objects across frames.</p></li><li class="list__item" id="z6sdncg_20"><p><span class="control" id="z6sdncg_26">Object Extraction</span>: The detected objects are extracted, including their bounding boxes, labels, and confidence scores. If pose detection is enabled, keypoints are also extracted.</p></li><li class="list__item" id="z6sdncg_21"><p><span class="control" id="z6sdncg_27">Object Counting</span>: The class counts the detected objects and measures various metrics such as the number of people, bikes, and cars.</p></li><li class="list__item" id="z6sdncg_22"><p><span class="control" id="z6sdncg_28">Scenario Processing</span>: Depending on the enabled scenarios, the class processes specific conditions: </p><ul class="list _bullet" id="z6sdncg_29"><li class="list__item" id="z6sdncg_30"><p><span class="control" id="z6sdncg_34">Zone Monitoring</span>: Detects objects within a predefined zone and measures their time spent in the zone.</p></li><li class="list__item" id="z6sdncg_31"><p><span class="control" id="z6sdncg_35">Parking Monitoring</span>: Detects parked objects and measures their time spent in the parking area.</p></li><li class="list__item" id="z6sdncg_32"><p><span class="control" id="z6sdncg_36">Door Monitoring</span>: Detects people entering or leaving through a door.</p></li><li class="list__item" id="z6sdncg_33"><p><span class="control" id="z6sdncg_37">Raise Hand Detection</span>: Detects people raising their hands.</p></li></ul></li><li class="list__item" id="z6sdncg_23"><p><span class="control" id="z6sdncg_38">Drawing Operations</span>: The class draws various information on the frame, including detected objects, scenario-specific indicators, and performance metrics.</p></li></ol></section><section class="chapter"><h3 id="reconnection-logic" data-toc="reconnection-logic">Reconnection Logic</h3><p id="z6sdncg_39">For network streams, the class implements reconnection logic to handle interruptions. If the stream becomes unavailable, it attempts to reconnect multiple times with a delay between attempts.</p></section><section class="chapter"><h3 id="handling-local-video-files" data-toc="handling-local-video-files">Handling Local Video Files</h3><p id="z6sdncg_40">For local video files, the class reads frames sequentially. If the end of the file is reached or an error occurs, it attempts to restart from the beginning of the file.</p></section></section><section class="chapter"><h2 id="example-usage" data-toc="example-usage">Example Usage</h2><div class="code-block" data-lang="python">
from fvgvisionai.config.app_settings import AppSettings
from fvgvisionai.input.stream_source.ultralytics_processor import UltralyticsProcessor
from fvgvisionai.common.video_observable import VideoObservable

app_settings = AppSettings(...)
video_observable = VideoObservable()
ultralytics_processor = UltralyticsProcessor(&quot;input_file.mp4&quot;, video_observable, app_settings)

exit_signal = threading.Event()
ultralytics_processor.process_source(exit_signal)
</div></section><section class="chapter"><h2 id="notes" data-toc="notes">Notes</h2><ul class="list _bullet" id="z6sdncg_42"><li class="list__item" id="z6sdncg_43"><p>Ensure the input file path or stream URL is correct and accessible.</p></li><li class="list__item" id="z6sdncg_44"><p>Properly configure the <code class="code" id="z6sdncg_45">AppSettings</code> class with necessary settings before initializing <code class="code" id="z6sdncg_46">UltralyticsProcessor</code>.</p></li></ul></section><div class="last-modified">Last modified: 14 November 2024</div><div data-feedback-placeholder="true"></div><div class="navigation-links _bottom"><a href="input.html" class="navigation-links__prev">Input module</a><a href="output.html" class="navigation-links__next">Output module</a></div></article><div id="disqus_thread"></div></div></section></main></div><script src="https://resources.jetbrains.com/writerside/apidoc/6.10.0-b518/app.js"></script></body></html>